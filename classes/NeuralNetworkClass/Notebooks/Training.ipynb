{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../NeuralNetworkClasses\")\n",
    "from extract_from_root import *\n",
    "from NN_class import *\n",
    "from dataset_loading import *\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import itertools\n",
    "import timeit\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-connected neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Data settings  ---\n",
      "{\n",
      "    \"GLOBAL\": {\n",
      "        \"transform_data\": false,\n",
      "        \"copy_to_device\": true\n",
      "    },\n",
      "    \"SCALERS\": {\n",
      "        \"X_data_scalers\": [\n",
      "            [\n",
      "                \"yeo-johnson\",\n",
      "                \"PowerTransformer()\"\n",
      "            ]\n",
      "        ],\n",
      "        \"Y_data_scalers\": [\n",
      "            [\n",
      "                \"yeo-johnson\",\n",
      "                \"PowerTransformer()\"\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    \"MACHINE_OPTIONS\": {\n",
      "        \"device\": \"mps\",\n",
      "        \"dtype_X\": \"torch.float32\",\n",
      "        \"dtype_Y\": \"torch.float32\",\n",
      "        \"amp\": false,\n",
      "        \"flexible_data\": false\n",
      "    },\n",
      "    \"OTHER\": {\n",
      "        \"num_workers\": 0\n",
      "    }\n",
      "} \n",
      "\n",
      "\n",
      "================ Data preparation ================\n",
      "\n",
      "No transformation is performed on training data!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "PyTorch is not linked with support for mps devices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22348/1980776632.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_data_X\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexample_data_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoading\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;31m### evaluate training and validation loss over epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data.local1/csonnab/PhD/classes/Neural-Network-Class/NeuralNetworkClasses/dataset_loading.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, training_data, validation_data, settings)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalingY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mScalingY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscalers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy_to_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"GLOBAL\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"copy_to_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"MACHINE_OPTIONS\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 self.datasetTS = dataset(self.scalingX.scale(training_data[0]),\n\u001b[0m\u001b[1;32m     93\u001b[0m                                          self.scalingY.scale(training_data[1]))\n\u001b[1;32m     94\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data.local1/csonnab/PhD/classes/Neural-Network-Class/NeuralNetworkClasses/dataset_loading.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0mtransformed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_to_dev\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflexible_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mtransformed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformed_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: PyTorch is not linked with support for mps devices"
     ]
    }
   ],
   "source": [
    "########### The neural network parametrization ###########\n",
    "\n",
    "### Neural Network\n",
    "\n",
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self, inch=1, window_size=11):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.window=window_size\n",
    "        \n",
    "        self.conv11 = nn.Conv2d(inch, 2, kernel_size=3, stride=1, padding=1)\n",
    "        #self.bn = nn.BatchNorm2d(2)        \n",
    "        self.conv21 = nn.Conv2d(2, 1, kernel_size=5, stride=1)\n",
    "        \n",
    "        self.fc31 = nn.Linear((window_size-4)**2, (window_size-4)**2)\n",
    "        self.fc41 = nn.Linear((window_size-4)**2, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \n",
    "        out = self.relu(self.conv11(X))\n",
    "        #out = self.bn(out)\n",
    "        # out = out+self.adjacent\n",
    "        out = self.relu(self.conv21(out))\n",
    "        out = out.reshape((-1,(self.window-4)**2))\n",
    "        out = self.tanh(self.fc31(out))\n",
    "        out = self.fc41(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "dict_net = {\n",
    "    \"GLOBAL\": {\n",
    "        \"epochs\": 3,\n",
    "        \"loss_function\": nn.MSELoss(),\n",
    "        \"optimizer\": optim.Adam,\n",
    "        \"scheduler\": optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        \"amp\": False, ### Mixed precision training\n",
    "        \"quantization\": False, ### Quantize the model to e.g. Int8\n",
    "        \"profiler\": True,\n",
    "    },\n",
    "    \"DATA_OPTIONS\": {\n",
    "        \"batchsize_schedule\": [0],\n",
    "        \"batchsize_training\": [None],\n",
    "        \"batchsize_validation\": None,\n",
    "        \"shuffle_every_epoch\": True,\n",
    "        \"num_workers\": 0,\n",
    "        \"pin_memory\": None,\n",
    "        \"copy_to_device\": False,\n",
    "    },\n",
    "    \"AMP_OPTIONS\" : {\n",
    "        \"dtype\": torch.float16,\n",
    "    },\n",
    "    \"QUANTIZATION_OPTIONS\" : {\n",
    "        \"ONNX\": {\n",
    "            \"weight_type\": QuantType.QInt8,\n",
    "            \"per_channel\": False,\n",
    "        },\n",
    "        \"PYTORCH\": {\n",
    "            \"dtype\": torch.qint8,\n",
    "        },\n",
    "    },\n",
    "    \"LOSS_OPTIONS\": {},\n",
    "    \"OPTIMIZER_OPTIONS\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0\n",
    "        },\n",
    "    \"SCHEDULER_OPTIONS\": {\n",
    "        \"patience\": 5,\n",
    "        \"factor\": 0.5\n",
    "        },\n",
    "    \"MACHINE_OPTIONS\": {\n",
    "        \"device\": 'mps',\n",
    "        \"cpu_threads\": None,\n",
    "        \"dtype\": torch.float32,\n",
    "    },\n",
    "    \"ONNX\": {\n",
    "        \"export_params\": True,\n",
    "        \"opset_version\": 17,\n",
    "        \"do_constant_folding\": True,\n",
    "        \"input_names\": [\"input\"],\n",
    "        \"output_names\": [\"output\"],\n",
    "        \"dynamic_axes\": {\n",
    "            \"input\": {0: \"batch_size\"},\n",
    "            \"output\": {0: \"batch_size\"},\n",
    "        },\n",
    "    },\n",
    "    \"PROFILER\": {\n",
    "        \"schedule\": torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "        \"on_trace_ready\": torch.profiler.tensorboard_trace_handler('./profiler_log'),\n",
    "        \"record_shapes\": True,\n",
    "        \"with_stack\": True,\n",
    "    },\n",
    "    \"OTHER\": {\n",
    "        \"verbose\": True,\n",
    "        \"n_samples\": np.infty,\n",
    "        \"save_at_epochs\": [1],\n",
    "    },\n",
    "}\n",
    "\n",
    "dict_data = {\n",
    "    \"GLOBAL\" : {\n",
    "        \"transform_data\" : False,\n",
    "        \"copy_to_device\" : True,\n",
    "    },\n",
    "    \"SCALERS\" : {\n",
    "        \"X_data_scalers\" : [('yeo-johnson', preprocessing.PowerTransformer(method='yeo-johnson', standardize=True))],\n",
    "        \"Y_data_scalers\" : [('yeo-johnson', preprocessing.PowerTransformer(method='yeo-johnson', standardize=True))],\n",
    "    },\n",
    "    \"MACHINE_OPTIONS\" : {\n",
    "        \"device\" : 'mps',\n",
    "        \"dtype_X\" : torch.float32,\n",
    "        \"dtype_Y\" : torch.float32\n",
    "    },\n",
    "    \"OTHER\" : {\n",
    "        \"num_workers\" : 0\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "### define the neural network\n",
    "NeuralNet = NN(network())\n",
    "\n",
    "### data preparation\n",
    "\n",
    "example_data_X = torch.tensor(np.random.random((1000,1,11,11)))\n",
    "example_data_Y = torch.tensor(np.random.random((1000,3)))\n",
    "X_train, X_test, y_train, y_test = train_test_split(example_data_X,example_data_Y,test_size=0.1,shuffle=True)\n",
    "\n",
    "data = DataLoading([X_train, y_train], [X_test, y_test], settings = dict_data)\n",
    "\n",
    "### evaluate training and validation loss over epochs                   \n",
    "NeuralNet.training(data, settings = dict_net)\n",
    "\n",
    "NeuralNet.save_net(avoid_q=True)\n",
    "NeuralNet.save_onnx(example_data=torch.tensor(X_train[:1]))\n",
    "NeuralNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = np.load(\"/lustre/alice/users/csonnab/PhD/jobs/clusterization/NN/training_data/normalized_qCenter/test/input_data.npy\")\n",
    "test_class = np.load(\"/lustre/alice/users/csonnab/PhD/jobs/clusterization/NN/training_data/normalized_qCenter/test/output_data_class.npy\").flatten()\n",
    "test_reg_label = np.loadtxt(\"/lustre/alice/users/csonnab/PhD/jobs/clusterization/NN/training_data/normalized_qCenter/test/output_data_reg.txt\", dtype=str)\n",
    "test_reg = np.load(\"/lustre/alice/users/csonnab/PhD/jobs/clusterization/NN/training_data/normalized_qCenter/test/output_data_reg.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mask = test_class > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = test_input[data_mask]\n",
    "test_class = test_class[data_mask]\n",
    "test_reg = test_reg[data_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_n_data = list()\n",
    "for i in range(5):\n",
    "    class_n_data.append(list())\n",
    "    for j, label in enumerate(test_reg_label):\n",
    "        for k in range(i):\n",
    "            if str(k) in label:\n",
    "                class_n_data[i].append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [0, 5, 10, 15, 20],\n",
       " [0, 1, 5, 6, 10, 11, 15, 16, 20, 21],\n",
       " [0, 1, 2, 5, 6, 7, 10, 11, 12, 15, 16, 17, 20, 21, 22],\n",
       " [0, 1, 2, 3, 5, 6, 7, 8, 10, 11, 12, 13, 15, 16, 17, 18, 20, 21, 22, 23]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_n_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [torch.tensor(5*i) for i in test_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [i for i in test_class]\n",
    "for i, lbl_idx in enumerate(test_class):\n",
    "    output_data[i] = test_reg[i, np.array(class_n_data[lbl_idx])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = list(zip(test_input, test_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network\n",
    "\n",
    "class network(nn.Module):\n",
    "\n",
    "    def __init__(self, model_classes):\n",
    "        super(network, self).__init__()\n",
    "        self.num_models = len(model_classes)\n",
    "        self.models = nn.ModuleList(model_classes)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        assert input_data[0].size(0) == input_data[1].size(0), \"Input and index array must have the same length.\"\n",
    "\n",
    "        # Use advanced indexing to select models based on the index array\n",
    "        selected_models = self.models[input_data[1]]\n",
    "\n",
    "        # Stack the selected models\n",
    "        stacked_models = nn.ModuleList(selected_models)\n",
    "\n",
    "        # Apply models to input data using parallel forward\n",
    "        output = nn.parallel.parallel_apply(stacked_models, input_data[1].chunk(input_data[0].size(0), dim=0))\n",
    "\n",
    "        return torch.cat(output, dim=0)\n",
    "\n",
    "\n",
    "class Model1(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(7*7*7, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 5)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        # out = self.quant(X)\n",
    "        X = X.view(-1,7*7*7)\n",
    "        out = self.relu(self.fc1(X))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.relu(self.fc5(out))\n",
    "        out = self.softmax(self.fc6(out))\n",
    "        return out\n",
    "\n",
    "class Model2(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(7*7*7, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.fc4 = nn.Linear(128, 64)\n",
    "        self.fc5 = nn.Linear(64, 32)\n",
    "        self.fc6 = nn.Linear(32, 10)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        # out = self.quant(X)\n",
    "        X = X.view(-1,7*7*7)\n",
    "        out = self.relu(self.fc1(X))\n",
    "        out = self.relu(self.fc2(out))\n",
    "        out = self.relu(self.fc3(out))\n",
    "        out = self.relu(self.fc4(out))\n",
    "        out = self.relu(self.fc5(out))\n",
    "        out = self.softmax(self.fc6(out))\n",
    "        return out\n",
    "\n",
    "# Create the model wrapper with three models\n",
    "model_classes = np.array([Model1(), Model2()])\n",
    "model_wrapper = network(model_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---  Network settings  ---\n",
      "{\n",
      "    \"GLOBAL\": {\n",
      "        \"epochs\": 3,\n",
      "        \"loss_function\": \"MSELoss()\",\n",
      "        \"optimizer\": \"<class 'torch.optim.adam.Adam'>\",\n",
      "        \"scheduler\": \"<class 'torch.optim.lr_scheduler.ReduceLROnPlateau'>\",\n",
      "        \"amp\": false,\n",
      "        \"quantization\": false,\n",
      "        \"profiler\": true\n",
      "    },\n",
      "    \"DATA_OPTIONS\": {\n",
      "        \"batchsize_schedule\": [\n",
      "            0\n",
      "        ],\n",
      "        \"batchsize_training\": [\n",
      "            null\n",
      "        ],\n",
      "        \"batchsize_validation\": null,\n",
      "        \"shuffle_every_epoch\": true,\n",
      "        \"num_workers\": 0,\n",
      "        \"pin_memory\": null,\n",
      "        \"copy_to_device\": false\n",
      "    },\n",
      "    \"AMP_OPTIONS\": {\n",
      "        \"dtype\": \"torch.float16\"\n",
      "    },\n",
      "    \"QUANTIZATION_OPTIONS\": {\n",
      "        \"ONNX\": {\n",
      "            \"weight_type\": \"QInt8\",\n",
      "            \"per_channel\": false\n",
      "        },\n",
      "        \"PYTORCH\": {\n",
      "            \"dtype\": \"torch.qint8\"\n",
      "        }\n",
      "    },\n",
      "    \"LOSS_OPTIONS\": {},\n",
      "    \"OPTIMIZER_OPTIONS\": {\n",
      "        \"lr\": 0.001,\n",
      "        \"weight_decay\": 0\n",
      "    },\n",
      "    \"SCHEDULER_OPTIONS\": {\n",
      "        \"patience\": 5,\n",
      "        \"factor\": 0.5\n",
      "    },\n",
      "    \"MACHINE_OPTIONS\": {\n",
      "        \"device\": null,\n",
      "        \"device_ids\": null,\n",
      "        \"cpu_threads\": null,\n",
      "        \"dtype\": \"torch.float32\"\n",
      "    },\n",
      "    \"ONNX\": {\n",
      "        \"export_params\": true,\n",
      "        \"opset_version\": 17,\n",
      "        \"do_constant_folding\": true,\n",
      "        \"input_names\": [\n",
      "            \"input\"\n",
      "        ],\n",
      "        \"output_names\": [\n",
      "            \"output\"\n",
      "        ],\n",
      "        \"dynamic_axes\": {\n",
      "            \"input\": {\n",
      "                \"0\": \"batch_size\"\n",
      "            },\n",
      "            \"output\": {\n",
      "                \"0\": \"batch_size\"\n",
      "            }\n",
      "        }\n",
      "    },\n",
      "    \"PROFILER\": {\n",
      "        \"schedule\": \"<function schedule.<locals>.schedule_fn at 0x7f91dc439510>\",\n",
      "        \"on_trace_ready\": \"<function tensorboard_trace_handler.<locals>.handler_fn at 0x7f91dc439598>\",\n",
      "        \"record_shapes\": true,\n",
      "        \"with_stack\": true\n",
      "    },\n",
      "    \"OTHER\": {\n",
      "        \"verbose\": true,\n",
      "        \"n_samples\": Infinity,\n",
      "        \"save_at_epochs\": [\n",
      "            1\n",
      "        ]\n",
      "    }\n",
      "} \n",
      "\n",
      "\n",
      "============ Neural Network training ============\n",
      "\n",
      "\n",
      "Running on CPU\n",
      "28 CPU threads\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataLoader' object has no attribute 'datasetTS'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30814/3499084941.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m### evaluate training and validation loss over epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavoid_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data.local1/csonnab/PhD/classes/Neural-Network-Class/NeuralNetworkClasses/NN_class.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, data, settings)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;31m### Checking if data was loaded properly and extracting the scalers ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_TS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasetTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen_VS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasetVS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataLoader' object has no attribute 'datasetTS'"
     ]
    }
   ],
   "source": [
    "########### The neural network parametrization ###########\n",
    "    \n",
    "dict_net = {\n",
    "    \"GLOBAL\": {\n",
    "        \"epochs\": 3,\n",
    "        \"loss_function\": nn.MSELoss(),\n",
    "        \"optimizer\": optim.Adam,\n",
    "        \"scheduler\": optim.lr_scheduler.ReduceLROnPlateau,\n",
    "        \"amp\": False, ### Mixed precision training\n",
    "        \"quantization\": False, ### Quantize the model to e.g. Int8\n",
    "        \"profiler\": True,\n",
    "    },\n",
    "    \"DATA_OPTIONS\": {\n",
    "        \"batchsize_schedule\": [0],\n",
    "        \"batchsize_training\": [None],\n",
    "        \"batchsize_validation\": None,\n",
    "        \"shuffle_every_epoch\": True,\n",
    "        \"num_workers\": 0,\n",
    "        \"pin_memory\": None,\n",
    "        \"copy_to_device\": False,\n",
    "    },\n",
    "    \"AMP_OPTIONS\" : {\n",
    "        \"dtype\": torch.float16,\n",
    "    },\n",
    "    \"QUANTIZATION_OPTIONS\" : {\n",
    "        \"ONNX\": {\n",
    "            \"weight_type\": QuantType.QInt8,\n",
    "            \"per_channel\": False,\n",
    "        },\n",
    "        \"PYTORCH\": {\n",
    "            \"dtype\": torch.qint8,\n",
    "        },\n",
    "    },\n",
    "    \"LOSS_OPTIONS\": {},\n",
    "    \"OPTIMIZER_OPTIONS\": {\n",
    "        \"lr\": 0.001,\n",
    "        \"weight_decay\": 0\n",
    "        },\n",
    "    \"SCHEDULER_OPTIONS\": {\n",
    "        \"patience\": 5,\n",
    "        \"factor\": 0.5\n",
    "        },\n",
    "    \"MACHINE_OPTIONS\": {\n",
    "        \"device\": None,\n",
    "        \"cpu_threads\": None,\n",
    "        \"dtype\": torch.float32,\n",
    "    },\n",
    "    \"ONNX\": {\n",
    "        \"export_params\": True,\n",
    "        \"opset_version\": 17,\n",
    "        \"do_constant_folding\": True,\n",
    "        \"input_names\": [\"input\"],\n",
    "        \"output_names\": [\"output\"],\n",
    "        \"dynamic_axes\": {\n",
    "            \"input\": {0: \"batch_size\"},\n",
    "            \"output\": {0: \"batch_size\"},\n",
    "        },\n",
    "    },\n",
    "    \"PROFILER\": {\n",
    "        \"schedule\": torch.profiler.schedule(wait=1, warmup=1, active=3, repeat=1),\n",
    "        \"on_trace_ready\": torch.profiler.tensorboard_trace_handler('./profiler_log'),\n",
    "        \"record_shapes\": True,\n",
    "        \"with_stack\": True,\n",
    "    },\n",
    "    \"OTHER\": {\n",
    "        \"verbose\": True,\n",
    "        \"n_samples\": np.infty,\n",
    "        \"save_at_epochs\": [1],\n",
    "    },\n",
    "}\n",
    "\n",
    "dict_data = {\n",
    "    \"GLOBAL\" : {\n",
    "        \"transform_data\" : False,\n",
    "        \"copy_to_device\" : True,\n",
    "    },\n",
    "    \"SCALERS\" : {\n",
    "        \"X_data_scalers\" : [('yeo-johnson', preprocessing.PowerTransformer(method='yeo-johnson', standardize=True))],\n",
    "        \"Y_data_scalers\" : [('yeo-johnson', preprocessing.PowerTransformer(method='yeo-johnson', standardize=True))],\n",
    "    },\n",
    "    \"MACHINE_OPTIONS\" : {\n",
    "        \"device\" : None,\n",
    "        \"dtype_X\" : torch.float32,\n",
    "        \"dtype_Y\" : torch.float32,\n",
    "        \"flexible_data\": True,\n",
    "    },\n",
    "    \"OTHER\" : {\n",
    "        \"num_workers\" : 0\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "### define the neural network\n",
    "NeuralNet = NN(network(model_classes))\n",
    "\n",
    "### data preparation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(input_data,output_data,test_size=0.1,shuffle=True)\n",
    "\n",
    "data = DataLoading([X_train, y_train], [X_test, y_test], settings = dict_data)\n",
    "\n",
    "### evaluate training and validation loss over epochs                   \n",
    "NeuralNet.training(data, settings = dict_net)\n",
    "\n",
    "NeuralNet.save_net(avoid_q=True)\n",
    "NeuralNet.save_onnx(example_data=torch.tensor(X_train[:1]))\n",
    "NeuralNet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
