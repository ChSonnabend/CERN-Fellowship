{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data and modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../NeuralNetworkClasses\")\n",
    "from extract_from_root import *\n",
    "from NN_class import *\n",
    "\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Subset\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import itertools\n",
    "import timeit\n",
    "import uproot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3689856, 10)\n"
     ]
    }
   ],
   "source": [
    "cload = load_tree()\n",
    "data_path=\"/lustre/alice/users/csonnab/TPC/NeuralNetworks/TrainingNetworks/LHC18b_FULL_GPU_SUPER_AGG_CUTS_3_smallNet/training_data.root\"\n",
    "data = cload.load(list_ignore=['fRelResoTPC','fNormMultTPC'], path=data_path)\n",
    "\n",
    "labels = np.array(data[0]).astype('U32')\n",
    "fit_data = np.array(data[1:]).astype(float)\n",
    "\n",
    "fit_data = fit_data[fit_data.T[0]!=0]\n",
    "\n",
    "print(np.shape(fit_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(fit_data)\n",
    "X = fit_data[:10000,2:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = fit_data[:10000,0]*fit_data[:10000,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.2\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_w_ind,y,test_size=TEST_SIZE,shuffle=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=TEST_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-27 12:02:54,586]\u001b[0m A new study created in memory with name: no-name-bfbfb110-36fc-42e8-86e3-5d76f51ad48b\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is the network structure:\n",
      "\n",
      "Fully-connected, Input size:  7 , Output size:  22 , activation:  ReLU()\n",
      "Fully-connected, Input size:  22 , Output size:  1 , activation:  Identity()\n",
      "\n",
      "Preparing training data...\n",
      "No transformation is performed on training data!\n",
      "Training data transformed.\n",
      "Continuing with validation data...\n",
      "\n",
      "No transformation is performed on validation data!\n",
      "Validation data transformed.\n",
      "Duration: 0.021 s\n",
      "\n",
      "Memory allocated:  Training data: 0.256 MiB, Validation data: 0.064 MiB\n",
      "\n",
      "Data is loaded, Training can begin!\n",
      "\n",
      "\n",
      "Running on CPU\n",
      "64 CPU threads\n",
      "\n",
      "Warning: No data-scalers found.\n",
      "Epoch  1 / 50 | Batch-size:  200 , Validation loss:  0.055472 , Training loss:  0.598322 , Execution time:  0.154 s\n",
      "Epoch  2 / 50 | Batch-size:  200 , Validation loss:  0.020025 , Training loss:  0.040414 , Execution time:  0.101 s\n",
      "Epoch  3 / 50 | Batch-size:  200 , Validation loss:  0.012065 , Training loss:  0.015746 , Execution time:  0.099 s\n",
      "Epoch  4 / 50 | Batch-size:  200 , Validation loss:  0.009921 , Training loss:  0.010678 , Execution time:  0.096 s\n",
      "Epoch  5 / 50 | Batch-size:  200 , Validation loss:  0.008999 , Training loss:  0.009156 , Execution time:  0.098 s\n",
      "Epoch  6 / 50 | Batch-size:  200 , Validation loss:  0.00837 , Training loss:  0.008439 , Execution time:  0.115 s\n",
      "Epoch  7 / 50 | Batch-size:  200 , Validation loss:  0.00793 , Training loss:  0.008034 , Execution time:  0.1 s\n",
      "Epoch  8 / 50 | Batch-size:  200 , Validation loss:  0.007599 , Training loss:  0.007598 , Execution time:  0.095 s\n",
      "Epoch  9 / 50 | Batch-size:  200 , Validation loss:  0.007344 , Training loss:  0.007338 , Execution time:  0.098 s\n",
      "Epoch  10 / 50 | Batch-size:  200 , Validation loss:  0.007154 , Training loss:  0.007118 , Execution time:  0.094 s\n",
      "Epoch  11 / 50 | Batch-size:  200 , Validation loss:  0.007209 , Training loss:  0.006936 , Execution time:  0.094 s\n",
      "Epoch  12 / 50 | Batch-size:  200 , Validation loss:  0.006815 , Training loss:  0.006782 , Execution time:  0.094 s\n",
      "Epoch  13 / 50 | Batch-size:  200 , Validation loss:  0.00723 , Training loss:  0.006702 , Execution time:  0.1 s\n",
      "Epoch  14 / 50 | Batch-size:  200 , Validation loss:  0.006781 , Training loss:  0.006628 , Execution time:  0.101 s\n",
      "Epoch  15 / 50 | Batch-size:  200 , Validation loss:  0.006519 , Training loss:  0.006441 , Execution time:  0.094 s\n",
      "Epoch  16 / 50 | Batch-size:  50 , Validation loss:  0.006371 , Training loss:  0.006748 , Execution time:  0.222 s\n",
      "Epoch  17 / 50 | Batch-size:  50 , Validation loss:  0.006526 , Training loss:  0.006207 , Execution time:  0.216 s\n",
      "Epoch  18 / 50 | Batch-size:  50 , Validation loss:  0.006338 , Training loss:  0.006293 , Execution time:  0.211 s\n",
      "Epoch  19 / 50 | Batch-size:  50 , Validation loss:  0.006261 , Training loss:  0.00616 , Execution time:  0.245 s\n",
      "Epoch  20 / 50 | Batch-size:  50 , Validation loss:  0.007011 , Training loss:  0.006235 , Execution time:  0.208 s\n",
      "Epoch  21 / 50 | Batch-size:  50 , Validation loss:  0.006112 , Training loss:  0.006294 , Execution time:  0.214 s\n",
      "Epoch  22 / 50 | Batch-size:  50 , Validation loss:  0.006471 , Training loss:  0.005926 , Execution time:  0.204 s\n",
      "Epoch  23 / 50 | Batch-size:  50 , Validation loss:  0.006386 , Training loss:  0.006222 , Execution time:  0.241 s\n",
      "Epoch  24 / 50 | Batch-size:  50 , Validation loss:  0.005845 , Training loss:  0.006132 , Execution time:  0.231 s\n",
      "Epoch  25 / 50 | Batch-size:  50 , Validation loss:  0.00584 , Training loss:  0.006101 , Execution time:  0.211 s\n",
      "Epoch  26 / 50 | Batch-size:  50 , Validation loss:  0.006115 , Training loss:  0.006104 , Execution time:  0.237 s\n",
      "Epoch  27 / 50 | Batch-size:  50 , Validation loss:  0.005799 , Training loss:  0.006069 , Execution time:  0.232 s\n",
      "Epoch  28 / 50 | Batch-size:  50 , Validation loss:  0.00584 , Training loss:  0.006222 , Execution time:  0.25 s\n",
      "Epoch  29 / 50 | Batch-size:  50 , Validation loss:  0.005849 , Training loss:  0.006334 , Execution time:  0.273 s\n",
      "Epoch  30 / 50 | Batch-size:  50 , Validation loss:  0.006325 , Training loss:  0.006154 , Execution time:  0.25 s\n",
      "Epoch  31 / 50 | Batch-size:  50 , Validation loss:  0.005799 , Training loss:  0.005944 , Execution time:  0.291 s\n",
      "Epoch  32 / 50 | Batch-size:  50 , Validation loss:  0.005845 , Training loss:  0.006343 , Execution time:  0.219 s\n",
      "Epoch  33 / 50 | Batch-size:  50 , Validation loss:  0.006287 , Training loss:  0.006317 , Execution time:  0.276 s\n",
      "Epoch  34 / 50 | Batch-size:  50 , Validation loss:  0.005901 , Training loss:  0.005723 , Execution time:  0.244 s\n",
      "Epoch  35 / 50 | Batch-size:  50 , Validation loss:  0.005767 , Training loss:  0.005811 , Execution time:  0.229 s\n",
      "Epoch  36 / 50 | Batch-size:  50 , Validation loss:  0.005765 , Training loss:  0.005764 , Execution time:  0.25 s\n",
      "Epoch  37 / 50 | Batch-size:  50 , Validation loss:  0.006429 , Training loss:  0.005737 , Execution time:  0.21 s\n",
      "Epoch  38 / 50 | Batch-size:  50 , Validation loss:  0.005759 , Training loss:  0.005728 , Execution time:  0.212 s\n",
      "Epoch  39 / 50 | Batch-size:  50 , Validation loss:  0.005855 , Training loss:  0.005658 , Execution time:  0.243 s\n",
      "Epoch  40 / 50 | Batch-size:  50 , Validation loss:  0.005971 , Training loss:  0.00587 , Execution time:  0.213 s\n",
      "1000 minibatches. Av. training loss: 0.00762054789630929\n",
      "Epoch  41 / 50 | Batch-size:  5 , Validation loss:  0.005925 , Training loss:  0.007202 , Execution time:  1.891 s\n",
      "1000 minibatches. Av. training loss: 0.0059304835677612576\n",
      "Epoch  42 / 50 | Batch-size:  5 , Validation loss:  0.006007 , Training loss:  0.006099 , Execution time:  1.663 s\n",
      "1000 minibatches. Av. training loss: 0.005588846020924393\n",
      "Epoch  43 / 50 | Batch-size:  5 , Validation loss:  0.005749 , Training loss:  0.00576 , Execution time:  1.779 s\n",
      "1000 minibatches. Av. training loss: 0.005394919993486838\n",
      "Epoch  44 / 50 | Batch-size:  5 , Validation loss:  0.005802 , Training loss:  0.005655 , Execution time:  1.996 s\n",
      "1000 minibatches. Av. training loss: 0.0055198919341273725\n",
      "Epoch  45 / 50 | Batch-size:  5 , Validation loss:  0.005737 , Training loss:  0.005578 , Execution time:  1.993 s\n",
      "1000 minibatches. Av. training loss: 0.005857124234935327\n",
      "Epoch  46 / 50 | Batch-size:  5 , Validation loss:  0.006084 , Training loss:  0.005569 , Execution time:  1.901 s\n",
      "1000 minibatches. Av. training loss: 0.005605966600509419\n",
      "Epoch  47 / 50 | Batch-size:  5 , Validation loss:  0.005903 , Training loss:  0.005598 , Execution time:  1.907 s\n",
      "1000 minibatches. Av. training loss: 0.005420364561956376\n",
      "Epoch  48 / 50 | Batch-size:  5 , Validation loss:  0.005751 , Training loss:  0.005578 , Execution time:  1.994 s\n",
      "1000 minibatches. Av. training loss: 0.005805445339006838\n",
      "Epoch  49 / 50 | Batch-size:  5 , Validation loss:  0.005891 , Training loss:  0.005603 , Execution time:  1.919 s\n",
      "1000 minibatches. Av. training loss: 0.0054985878372463045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-04-27 12:03:21,061]\u001b[0m Trial 0 finished with value: 0.005748252384364605 and parameters: {'num_layers': 0, 'num_neurons_layer': 22}. Best is trial 0 with value: 0.005748252384364605.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  50 / 50 | Batch-size:  5 , Validation loss:  0.005748 , Training loss:  0.005591 , Execution time:  1.975 s\n",
      "\n",
      "Training finished!\n",
      "\n",
      "\n",
      "This is the network structure:\n",
      "\n",
      "Fully-connected, Input size:  7 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  19 , activation:  ReLU()\n",
      "Fully-connected, Input size:  19 , Output size:  1 , activation:  Identity()\n",
      "\n",
      "Preparing training data...\n",
      "No transformation is performed on training data!\n",
      "Training data transformed.\n",
      "Continuing with validation data...\n",
      "\n",
      "No transformation is performed on validation data!\n",
      "Validation data transformed.\n",
      "Duration: 0.143 s\n",
      "\n",
      "Memory allocated:  Training data: 0.256 MiB, Validation data: 0.064 MiB\n",
      "\n",
      "Data is loaded, Training can begin!\n",
      "\n",
      "\n",
      "Running on CPU\n",
      "64 CPU threads\n",
      "\n",
      "Warning: No data-scalers found.\n",
      "Epoch  1 / 50 | Batch-size:  200 , Validation loss:  0.006175 , Training loss:  0.116046 , Execution time:  0.674 s\n",
      "Epoch  2 / 50 | Batch-size:  200 , Validation loss:  0.005759 , Training loss:  0.005636 , Execution time:  0.674 s\n",
      "Epoch  3 / 50 | Batch-size:  200 , Validation loss:  0.005715 , Training loss:  0.005534 , Execution time:  0.604 s\n",
      "Epoch  4 / 50 | Batch-size:  200 , Validation loss:  0.005875 , Training loss:  0.00553 , Execution time:  0.598 s\n",
      "Epoch  5 / 50 | Batch-size:  200 , Validation loss:  0.005715 , Training loss:  0.005559 , Execution time:  0.61 s\n",
      "Epoch  6 / 50 | Batch-size:  200 , Validation loss:  0.005724 , Training loss:  0.005533 , Execution time:  0.623 s\n",
      "Epoch  7 / 50 | Batch-size:  200 , Validation loss:  0.006056 , Training loss:  0.005575 , Execution time:  0.624 s\n",
      "Epoch  8 / 50 | Batch-size:  200 , Validation loss:  0.006004 , Training loss:  0.005616 , Execution time:  0.607 s\n",
      "Epoch  9 / 50 | Batch-size:  200 , Validation loss:  0.005714 , Training loss:  0.005634 , Execution time:  0.611 s\n",
      "Epoch  10 / 50 | Batch-size:  200 , Validation loss:  0.005813 , Training loss:  0.005608 , Execution time:  0.605 s\n",
      "Epoch  11 / 50 | Batch-size:  200 , Validation loss:  0.005738 , Training loss:  0.005702 , Execution time:  0.604 s\n",
      "Epoch  12 / 50 | Batch-size:  200 , Validation loss:  0.005749 , Training loss:  0.005581 , Execution time:  0.659 s\n",
      "Epoch  13 / 50 | Batch-size:  200 , Validation loss:  0.005726 , Training loss:  0.005523 , Execution time:  0.67 s\n",
      "Epoch  14 / 50 | Batch-size:  200 , Validation loss:  0.005715 , Training loss:  0.005533 , Execution time:  0.589 s\n",
      "Epoch  15 / 50 | Batch-size:  200 , Validation loss:  0.005871 , Training loss:  0.005544 , Execution time:  0.599 s\n",
      "Epoch  16 / 50 | Batch-size:  50 , Validation loss:  0.005714 , Training loss:  0.005584 , Execution time:  2.11 s\n",
      "Epoch  17 / 50 | Batch-size:  50 , Validation loss:  0.005739 , Training loss:  0.005586 , Execution time:  2.388 s\n",
      "Epoch  18 / 50 | Batch-size:  50 , Validation loss:  0.005724 , Training loss:  0.005592 , Execution time:  2.381 s\n",
      "Epoch  19 / 50 | Batch-size:  50 , Validation loss:  0.005815 , Training loss:  0.005589 , Execution time:  2.29 s\n",
      "Epoch  20 / 50 | Batch-size:  50 , Validation loss:  0.005714 , Training loss:  0.005651 , Execution time:  2.31 s\n",
      "Epoch  21 / 50 | Batch-size:  50 , Validation loss:  0.005724 , Training loss:  0.005583 , Execution time:  2.204 s\n",
      "Epoch  22 / 50 | Batch-size:  50 , Validation loss:  0.00573 , Training loss:  0.005566 , Execution time:  2.431 s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-9c9f637e2387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params\u001b[0m  \u001b[0;31m# E.g. {'x': 2.002108042}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-9c9f637e2387>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     39\u001b[0m     NeuralNet.training(data, epochs=N_EPOCHS, epochs_ls=EPOCH_LS, weights = False,\n\u001b[1;32m     40\u001b[0m                        \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSCHEDULER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLOSS_FUNCTION\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                        verbose=True, learning_rate=LEARNING_RATE, patience=PATIENCE, factor=FACTOR)\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/lustre/alice/users/csonnab/TPC/NeuralNetworks/Neural-Networks/NeuralNetworkClasses/NN_class.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self, data, epochs, epochs_ls, optimizer, scheduler, learning_rate, weight_decay, loss_function, weights, verbose, nsamples, set_num_threads, patience, factor)\u001b[0m\n\u001b[1;32m    271\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    142\u001b[0m                    \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                    eps=group['eps'])\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    num_layers = trial.suggest_int('num_layers', 0, 30)\n",
    "    num_neurons_layer = trial.suggest_int('num_neurons_layer', 0, 30)\n",
    "    \n",
    "    X_SCALERS = []\n",
    "    Y_SCALERS = []\n",
    "    \n",
    "    H_SIZES = list(itertools.chain(*[[[7,num_neurons_layer]],[[num_neurons_layer,num_neurons_layer]]*num_layers,[[num_neurons_layer,1]]]))\n",
    "    LAYER_TYPES = list(itertools.chain(*[['fc'],['fc']*(len(H_SIZES)-2), ['fc']]))\n",
    "    WEIGHT_INIT = nn.init.xavier_normal_\n",
    "    GAIN = 5/3\n",
    "    ACTIVATION =list(itertools.chain(*[[nn.ReLU()]*(len(H_SIZES)-1), [nn.Identity()]]))\n",
    "    \n",
    "    BATCH_SIZES = [200,50,5]\n",
    "    N_EPOCHS = 50\n",
    "    EPOCH_LS = [0,15,40]\n",
    "\n",
    "\n",
    "    OPTIMIZER = optim.Adam\n",
    "    WEIGHT_DECAY = 0\n",
    "    SCHEDULER = optim.lr_scheduler.ReduceLROnPlateau \n",
    "    LOSS_FUNCTION = weighted_mse_loss\n",
    "    LEARNING_RATE = 0.01\n",
    "    \n",
    "    NUM_THREADS = 0\n",
    "    PATIENCE = 5\n",
    "    FACTOR=0.5\n",
    "    \n",
    "    net = General_NN(params = H_SIZES, layer_types = LAYER_TYPES, act_func =ACTIVATION, w_init = WEIGHT_INIT, verbose=True)#, gain=GAIN)\n",
    "    \n",
    "    NeuralNet = NN(net)\n",
    "\n",
    "    data = DataLoading([X_train, y_train], [X_test, y_test], \n",
    "                        X_data_scalers=X_SCALERS, y_data_scalers=Y_SCALERS,\n",
    "                        batch_sizes=BATCH_SIZES, shuffle_every_epoch=True,\n",
    "                        transformTS=False, transformVS=False)\n",
    "    \n",
    "    NeuralNet.training(data, epochs=N_EPOCHS, epochs_ls=EPOCH_LS, weights = False,\n",
    "                       optimizer=OPTIMIZER,scheduler=SCHEDULER, loss_function=LOSS_FUNCTION, \n",
    "                       verbose=True, learning_rate=LEARNING_RATE, patience=PATIENCE, factor=FACTOR)\n",
    "\n",
    "    return NeuralNet.validation_loss[-1]\n",
    "\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "study.best_params  # E.g. {'x': 2.002108042}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
